{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSAeKnPAaeF1",
        "outputId": "834def5e-8177-4cef-c5ab-240d4173a1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Loading and preprocessing data...\n",
            "Applying SMOTE for class balancing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-19 03:47:20,532] A new study created in memory with name: no-name-afe061d9-4716-4751-acb3-22abee43f831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing hyperparameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-19 03:53:07,901] Trial 0 finished with value: 0.5424138631758261 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.04614351715100795, 'subsample': 0.6814878649620414, 'colsample_bytree': 0.6272369503240611, 'gamma': 0.02971621495434198, 'reg_lambda': 0.002162021428336622, 'reg_alpha': 0.23462801694990357}. Best is trial 0 with value: 0.5424138631758261.\n",
            "[I 2025-02-19 04:04:22,882] Trial 1 finished with value: 0.7568738913029235 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.0896345752793483, 'subsample': 0.700453763579332, 'colsample_bytree': 0.8328211994431439, 'gamma': 0.05347100476941819, 'reg_lambda': 0.03770223711662937, 'reg_alpha': 0.3800997235144518}. Best is trial 1 with value: 0.7568738913029235.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.0896345752793483, 'subsample': 0.700453763579332, 'colsample_bytree': 0.8328211994431439, 'gamma': 0.05347100476941819, 'reg_lambda': 0.03770223711662937, 'reg_alpha': 0.3800997235144518}\n",
            "Best CV accuracy: 0.7569\n",
            "Training final model...\n",
            "\n",
            "Final Model Accuracy: 0.7829\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       219\n",
            "           1       1.00      1.00      1.00       220\n",
            "           2       0.65      0.68      0.67       220\n",
            "           3       0.65      0.65      0.65       220\n",
            "           4       0.99      0.99      0.99       219\n",
            "           5       0.95      0.98      0.96       220\n",
            "           6       0.65      0.59      0.61       220\n",
            "           7       0.61      0.61      0.61       220\n",
            "           8       0.99      1.00      0.99       220\n",
            "           9       0.64      0.59      0.61       219\n",
            "          10       0.97      1.00      0.98       219\n",
            "          11       0.70      0.70      0.70       220\n",
            "          12       0.98      0.99      0.98       219\n",
            "          13       0.97      0.99      0.98       220\n",
            "          14       0.59      0.57      0.58       220\n",
            "          15       0.44      0.58      0.50       220\n",
            "          16       0.65      0.53      0.58       220\n",
            "          17       0.65      0.65      0.65       220\n",
            "          18       0.60      0.61      0.60       219\n",
            "          19       0.98      0.98      0.98       220\n",
            "          20       0.64      0.62      0.63       220\n",
            "          21       0.95      1.00      0.97       220\n",
            "          22       0.63      0.72      0.67       219\n",
            "          23       0.66      0.59      0.62       220\n",
            "          24       0.68      0.59      0.63       220\n",
            "          25       1.00      0.98      0.99       220\n",
            "          26       0.97      0.95      0.96       220\n",
            "          27       0.62      0.56      0.59       220\n",
            "          28       0.93      1.00      0.96       220\n",
            "          29       0.81      0.75      0.78       220\n",
            "          30       0.64      0.65      0.65       220\n",
            "          31       0.63      0.62      0.62       219\n",
            "          32       0.96      1.00      0.98       220\n",
            "          33       0.61      0.61      0.61       219\n",
            "          34       0.96      1.00      0.98       220\n",
            "          35       0.97      0.97      0.97       220\n",
            "          36       0.98      1.00      0.99       220\n",
            "          37       0.97      0.98      0.97       220\n",
            "          38       0.98      0.99      0.98       220\n",
            "          39       0.44      0.50      0.47       220\n",
            "          40       0.58      0.56      0.57       220\n",
            "          41       0.95      0.98      0.96       220\n",
            "          42       0.95      1.00      0.97       220\n",
            "          43       0.58      0.50      0.54       220\n",
            "          44       0.99      0.95      0.97       219\n",
            "          45       0.96      1.00      0.98       220\n",
            "          46       0.92      0.88      0.90       220\n",
            "          47       0.45      0.37      0.40       219\n",
            "          48       0.89      0.90      0.90       220\n",
            "          49       0.79      0.79      0.79       220\n",
            "          50       0.62      0.59      0.60       220\n",
            "          51       0.63      0.65      0.64       220\n",
            "          52       0.43      0.51      0.47       220\n",
            "          53       0.96      1.00      0.98       219\n",
            "          54       0.95      0.97      0.96       220\n",
            "          55       0.57      0.60      0.59       220\n",
            "          56       0.65      0.67      0.66       220\n",
            "          57       0.97      0.99      0.98       220\n",
            "          58       1.00      1.00      1.00       220\n",
            "          59       0.94      1.00      0.97       219\n",
            "          60       0.95      1.00      0.98       220\n",
            "          61       0.95      0.98      0.96       220\n",
            "          62       0.67      0.64      0.65       220\n",
            "          63       0.59      0.67      0.63       220\n",
            "          64       0.62      0.62      0.62       220\n",
            "          65       0.75      0.60      0.67       220\n",
            "          66       0.62      0.59      0.60       220\n",
            "          67       0.78      0.83      0.81       220\n",
            "          68       0.59      0.48      0.53       220\n",
            "\n",
            "    accuracy                           0.78     15167\n",
            "   macro avg       0.78      0.78      0.78     15167\n",
            "weighted avg       0.78      0.78      0.78     15167\n",
            "\n",
            "Saving model and preprocessors...\n",
            "\n",
            "Making predictions for sample inputs:\n",
            "\n",
            "Sample 1:\n",
            "Input Parameters: {'Nitrogen': 90, 'Phosphorus': 42, 'Potassium': 43, 'Temperature': 25, 'Humidity': 82, 'pH': 6.5, 'Soil Type': 'Clayey', 'Period of Month': 'Kharif'}\n",
            "Predicted Crop: Rice\n",
            "Confidence: 64.16%\n",
            "\n",
            "Top 3 Crop Recommendations:\n",
            "Rice: 64.16%\n",
            "Maize: 23.56%\n",
            "Brinjal: 8.10%\n",
            "\n",
            "Sample 2:\n",
            "Input Parameters: {'Nitrogen': 120, 'Phosphorus': 35, 'Potassium': 30, 'Temperature': 28, 'Humidity': 75, 'pH': 7.0, 'Soil Type': 'Loamy', 'Period of Month': 'Rabi'}\n",
            "Predicted Crop: Potato\n",
            "Confidence: 72.21%\n",
            "\n",
            "Top 3 Crop Recommendations:\n",
            "Potato: 72.21%\n",
            "Wheat: 14.60%\n",
            "Peas: 6.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install xgboost\n",
        "!pip install imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class CropPredictor:\n",
        "    def __init__(self):\n",
        "        self.label_encoder_soil = LabelEncoder()\n",
        "        self.label_encoder_period = LabelEncoder()\n",
        "        self.label_encoder_crop = LabelEncoder()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def preprocess_data(self, data, is_training=True):\n",
        "        # Create a copy of the data\n",
        "        processed_data = data.copy()\n",
        "\n",
        "        if is_training:\n",
        "            # Fit and transform for training data\n",
        "            processed_data['Soil Type'] = self.label_encoder_soil.fit_transform(processed_data['Soil Type'])\n",
        "            processed_data['Period of Month'] = self.label_encoder_period.fit_transform(processed_data['Period of Month'])\n",
        "        else:\n",
        "            # Transform only for prediction data\n",
        "            processed_data['Soil Type'] = self.label_encoder_soil.transform(processed_data['Soil Type'])\n",
        "            processed_data['Period of Month'] = self.label_encoder_period.transform(processed_data['Period of Month'])\n",
        "\n",
        "        # Feature Engineering\n",
        "        processed_data['NPK_Ratio'] = processed_data['Nitrogen'] / (processed_data['Phosphorus'] + processed_data['Potassium'])\n",
        "        processed_data['Temp_Humidity_Index'] = processed_data['Temperature'] * processed_data['Humidity'] / 100\n",
        "        processed_data['Soil_Moisture_Index'] = np.log1p(processed_data['Temperature'] * processed_data['Humidity'])\n",
        "        processed_data['NPK_TH_Index'] = processed_data['NPK_Ratio'] * processed_data['Temp_Humidity_Index']\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def objective(self, trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
        "            'gamma': trial.suggest_float('gamma', 0.01, 0.5, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True)\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBClassifier(**params, objective='multi:softprob', eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "        cv_scores = []\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
        "            X_fold_train, X_fold_val = self.X_train[train_idx], self.X_train[val_idx]\n",
        "            y_fold_train, y_fold_val = self.y_train[train_idx], self.y_train[val_idx]\n",
        "\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "            preds = model.predict(X_fold_val)\n",
        "            cv_scores.append(accuracy_score(y_fold_val, preds))\n",
        "\n",
        "        return np.mean(cv_scores)\n",
        "\n",
        "    def train(self, data_path):\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        data = pd.read_csv(data_path)\n",
        "\n",
        "        # Encode target variable\n",
        "        y = self.label_encoder_crop.fit_transform(data['Crop'])\n",
        "\n",
        "        # Preprocess features\n",
        "        X = self.preprocess_data(data.drop('Crop', axis=1), is_training=True)\n",
        "        self.feature_names = X.columns.tolist()\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = X.values\n",
        "\n",
        "        print(\"Applying SMOTE for class balancing...\")\n",
        "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "        self.X_train, X_test, self.y_train, y_test = train_test_split(\n",
        "            X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        X_test = self.scaler.transform(X_test)\n",
        "\n",
        "        print(\"Optimizing hyperparameters...\")\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(self.objective, n_trials=30, timeout=600)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Best CV accuracy: {study.best_value:.4f}\")\n",
        "\n",
        "        print(\"Training final model...\")\n",
        "        self.model = xgb.XGBClassifier(\n",
        "            **best_params,\n",
        "            objective='multi:softprob',\n",
        "            eval_metric='mlogloss',\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        preds = self.model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, preds)\n",
        "        print(f\"\\nFinal Model Accuracy: {accuracy:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, preds))\n",
        "\n",
        "        print(\"Saving model and preprocessors...\")\n",
        "        with open('crop_prediction_model.pkl', 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'model': self.model,\n",
        "                'label_encoder_crop': self.label_encoder_crop,\n",
        "                'label_encoder_soil': self.label_encoder_soil,\n",
        "                'label_encoder_period': self.label_encoder_period,\n",
        "                'scaler': self.scaler,\n",
        "                'feature_names': self.feature_names\n",
        "            }, f)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def predict_crop(self, input_data):\n",
        "        # Convert input to DataFrame if it's a dictionary\n",
        "        if isinstance(input_data, dict):\n",
        "            input_data = pd.DataFrame([input_data])\n",
        "\n",
        "        # Preprocess the input data\n",
        "        processed_data = self.preprocess_data(input_data, is_training=False)\n",
        "\n",
        "        # Ensure column order matches training data\n",
        "        processed_data = processed_data[self.feature_names]\n",
        "\n",
        "        # Scale the features\n",
        "        scaled_data = self.scaler.transform(processed_data)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = self.model.predict(scaled_data)\n",
        "        probabilities = self.model.predict_proba(scaled_data)\n",
        "\n",
        "        predicted_crop = self.label_encoder_crop.inverse_transform(prediction)\n",
        "        return predicted_crop[0], probabilities[0]\n",
        "\n",
        "def main():\n",
        "    predictor = CropPredictor()\n",
        "    accuracy = predictor.train('updated_crop_data_cleaned.csv')\n",
        "\n",
        "    # Sample inputs\n",
        "    sample_inputs = [\n",
        "        {\n",
        "            'Nitrogen': 90,\n",
        "            'Phosphorus': 42,\n",
        "            'Potassium': 43,\n",
        "            'Temperature': 25,\n",
        "            'Humidity': 82,\n",
        "            'pH': 6.5,\n",
        "            'Soil Type': 'Clayey',\n",
        "            'Period of Month': 'Kharif'\n",
        "        },\n",
        "        {\n",
        "            'Nitrogen': 120,\n",
        "            'Phosphorus': 35,\n",
        "            'Potassium': 30,\n",
        "            'Temperature': 28,\n",
        "            'Humidity': 75,\n",
        "            'pH': 7.0,\n",
        "            'Soil Type': 'Loamy',\n",
        "            'Period of Month': 'Rabi'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nMaking predictions for sample inputs:\")\n",
        "    for i, sample in enumerate(sample_inputs, 1):\n",
        "        predicted_crop, probabilities = predictor.predict_crop(sample)\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"Input Parameters: {sample}\")\n",
        "        print(f\"Predicted Crop: {predicted_crop}\")\n",
        "        print(f\"Confidence: {max(probabilities)*100:.2f}%\")\n",
        "\n",
        "        top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
        "        print(\"\\nTop 3 Crop Recommendations:\")\n",
        "        for idx in top_3_indices:\n",
        "            crop_name = predictor.label_encoder_crop.inverse_transform([idx])[0]\n",
        "            print(f\"{crop_name}: {probabilities[idx]*100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}